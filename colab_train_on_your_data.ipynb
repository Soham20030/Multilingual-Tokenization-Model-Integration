{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Train BLOOMZ-560M Adapter on YOUR Multilingual Data\n",
        "\n",
        "## üìù Instructions:\n",
        "\n",
        "### Step 1: Upload Your Data (GOOGLE DRIVE - RECOMMENDED!)\n",
        "\n",
        "**Option A: Google Drive (Faster & Easier)** ‚úÖ\n",
        "1. Upload ALL `.txt` files from your `data/training/` folder to Google Drive:\n",
        "   - Create a folder in Google Drive called `multilingual_training_data`\n",
        "   - Upload: `hi_train.txt`, `bn_train.txt`, `ta_train.txt`, `te_train.txt`, etc.\n",
        "2. In Cell 3 below, keep `USE_GOOGLE_DRIVE = True`\n",
        "3. Update the `data_folder` path to match your Drive folder name\n",
        "\n",
        "**Option B: Direct Upload to Colab** (Slower)\n",
        "1. Set `USE_GOOGLE_DRIVE = False` in Cell 3\n",
        "2. Click the **folder icon** on the left sidebar in Colab\n",
        "3. Create a folder called `training_data`\n",
        "4. Manually upload all `.txt` files (this can take 10-15 minutes)\n",
        "\n",
        "### Step 2: Enable GPU\n",
        "- Click **Runtime ‚Üí Change runtime type**\n",
        "- Select **GPU** (T4 is free)\n",
        "- Click **Save**\n",
        "\n",
        "### Step 3: Run All Cells\n",
        "- Click **Runtime ‚Üí Run all**\n",
        "- Wait ~20-30 minutes for training\n",
        "\n",
        "### Step 4: Download\n",
        "- Download the `gurukul_adapter.zip` file\n",
        "- Extract it to `adapters/gurukul_lite/` on your PC\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Install Dependencies & Check GPU\n",
        "print(\"üì¶ Installing dependencies...\\n\")\n",
        "!pip install -q transformers datasets peft accelerate bitsandbytes scipy\n",
        "\n",
        "import torch\n",
        "print(f\"\\n‚úÖ GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è WARNING: No GPU detected. Training will be VERY slow!\")\n",
        "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí Select GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Load Model & Prepare Training\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "from datasets import Dataset\n",
        "import glob\n",
        "import random\n",
        "\n",
        "print(\"ü§ñ Loading BLOOMZ-560M...\\n\")\n",
        "\n",
        "# Quantization config for memory efficiency\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model_name = \"bigscience/bloomz-560m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Set padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"‚úÖ Model loaded!\\n\")\n",
        "\n",
        "# Training config (adjust these if needed)\n",
        "config = {\n",
        "    'max_samples': 5000,      # Total samples across all languages\n",
        "    'max_length': 256,        # Max token length\n",
        "    'num_epochs': 3,          # Training epochs\n",
        "    'batch_size': 4,          # Batch size\n",
        "    'learning_rate': 3e-4,    # Learning rate\n",
        "    'lora_r': 8,              # LoRA rank\n",
        "    'lora_alpha': 16,         # LoRA alpha\n",
        "    'lora_dropout': 0.05      # LoRA dropout\n",
        "}\n",
        "\n",
        "print(f\"üìä Training Config:\")\n",
        "for k, v in config.items():\n",
        "    print(f\"   {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Load YOUR Training Data\n",
        "import os\n",
        "\n",
        "print(\"üìÇ Loading your training data...\\n\")\n",
        "\n",
        "# OPTION 1: Google Drive (RECOMMENDED - faster!)\n",
        "# Uncomment the lines below if you uploaded your data to Google Drive\n",
        "USE_GOOGLE_DRIVE = True  # Change to False if uploading directly to Colab\n",
        "\n",
        "if USE_GOOGLE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    print(\"üîó Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # CHANGE THIS PATH to where you uploaded your training files in Google Drive\n",
        "    # Example: If you created a folder \"My Drive/multilingual_training_data/\"\n",
        "    data_folder = \"/content/drive/MyDrive/multilingual_training_data\"\n",
        "    \n",
        "    print(f\"‚úÖ Google Drive mounted!\")\n",
        "    print(f\"   Looking for data in: {data_folder}\\n\")\n",
        "else:\n",
        "    # OPTION 2: Direct upload to Colab\n",
        "    data_folder = \"training_data\"\n",
        "\n",
        "# Check if data folder exists\n",
        "if not os.path.exists(data_folder):\n",
        "    print(f\"‚ùå ERROR: Folder '{data_folder}' not found!\")\n",
        "    if USE_GOOGLE_DRIVE:\n",
        "        print(\"\\n‚ö†Ô∏è Please:\")\n",
        "        print(\"   1. Upload your training .txt files to Google Drive\")\n",
        "        print(\"   2. Update 'data_folder' path above to match your Drive folder\")\n",
        "        print(\"   3. Re-run this cell\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è Please:\")\n",
        "        print(\"   1. Click the folder icon on the left\")\n",
        "        print(\"   2. Create a folder called 'training_data'\")\n",
        "        print(\"   3. Upload all your .txt files from data/training/\")\n",
        "        print(\"   4. Re-run this cell\")\n",
        "    raise FileNotFoundError(f\"Missing {data_folder}/\")\n",
        "\n",
        "# Find all .txt files\n",
        "txt_files = glob.glob(f\"{data_folder}/*.txt\")\n",
        "\n",
        "if not txt_files:\n",
        "    print(f\"‚ùå ERROR: No .txt files found in {data_folder}/\")\n",
        "    print(\"\\n‚ö†Ô∏è Please upload your training files!\")\n",
        "    raise FileNotFoundError(\"No training data found\")\n",
        "\n",
        "print(f\"‚úÖ Found {len(txt_files)} files:\\n\")\n",
        "for f in txt_files:\n",
        "    size_mb = os.path.getsize(f) / 1e6\n",
        "    print(f\"   ‚Ä¢ {os.path.basename(f):20s} ({size_mb:.2f} MB)\")\n",
        "\n",
        "# Load and combine data from all files\n",
        "all_texts = []\n",
        "lang_counts = {}\n",
        "\n",
        "for filepath in txt_files:\n",
        "    lang_code = os.path.basename(filepath).split('_')[0]  # e.g., 'hi' from 'hi_train.txt'\n",
        "    \n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        lines = [line.strip() for line in f if line.strip()]\n",
        "    \n",
        "    # Sample from each file\n",
        "    samples_per_file = min(len(lines), config['max_samples'] // len(txt_files))\n",
        "    sampled = random.sample(lines, samples_per_file)\n",
        "    \n",
        "    all_texts.extend(sampled)\n",
        "    lang_counts[lang_code] = len(sampled)\n",
        "    \n",
        "print(f\"\\nüìä Samples per language:\")\n",
        "for lang, count in sorted(lang_counts.items()):\n",
        "    print(f\"   {lang}: {count:,}\")\n",
        "\n",
        "print(f\"\\nüìà Total samples: {len(all_texts):,}\")\n",
        "\n",
        "# Shuffle\n",
        "random.shuffle(all_texts)\n",
        "\n",
        "# Create dataset\n",
        "dataset = Dataset.from_dict({'text': all_texts})\n",
        "print(f\"\\n‚úÖ Dataset created with {len(dataset):,} examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Tokenize Data\n",
        "print(\"üî§ Tokenizing data...\\n\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # Simple tokenization - data collator handles padding AND label creation\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        max_length=config['max_length']\n",
        "    )\n",
        "\n",
        "# Tokenize in batches\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['text'],\n",
        "    desc=\"Tokenizing\",\n",
        "    batch_size=100  # Process in larger batches for speed\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Tokenization complete!\")\n",
        "print(f\"   Dataset size: {len(tokenized_dataset):,} examples\")\n",
        "\n",
        "# Verify tokenization\n",
        "sample = tokenized_dataset[0]\n",
        "print(f\"   Sample input length: {len(sample['input_ids'])}\")\n",
        "print(f\"   First 10 tokens: {sample['input_ids'][:10]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Apply LoRA Adapter\n",
        "print(\"üîß Applying LoRA adapter...\\n\")\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA configuration (PROVEN settings for BLOOM)\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=config['lora_r'],\n",
        "    lora_alpha=config['lora_alpha'],\n",
        "    lora_dropout=config['lora_dropout'],\n",
        "    target_modules=['query_key_value', 'dense', 'dense_h_to_4h', 'dense_4h_to_h'],\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"‚úÖ LoRA applied!\")\n",
        "print(f\"   Trainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "print(f\"   Total params: {total_params:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Train the Model!\n",
        "print(\"üöÄ Starting training...\\n\")\n",
        "print(\"‚è±Ô∏è This will take 20-30 minutes depending on GPU.\\n\")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./adapter_training\",\n",
        "    num_train_epochs=config['num_epochs'],\n",
        "    per_device_train_batch_size=config['batch_size'],\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=config['learning_rate'],\n",
        "    fp16=True,  # Mixed precision for speed\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    warmup_steps=100,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",\n",
        "    optim=\"adamw_torch\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    gradient_checkpointing=True,\n",
        "    dataloader_num_workers=0,  # Use 0 to avoid multiprocessing issues\n",
        ")\n",
        "\n",
        "# Data collator - use the built-in one, it's simpler and works!\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Causal LM (not masked LM)\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Train!\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ TRAINING COMPLETE!\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Test the Trained Adapter\n",
        "print(\"üß™ Testing the trained adapter...\\n\")\n",
        "\n",
        "test_prompts = [\n",
        "    \"Translate to Hindi: Hello friend, how are you?\",\n",
        "    \"Translate to Bengali: Good morning.\",\n",
        "    \"Translate to Tamil: Thank you.\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            min_new_tokens=10,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            no_repeat_ngram_size=3,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"{i}. Prompt: {prompt}\")\n",
        "    print(f\"   Output: {generated}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Save & Package Adapter\n",
        "print(\"üíæ Saving adapter...\\n\")\n",
        "\n",
        "# Save adapter\n",
        "model.save_pretrained(\"gurukul_adapter\")\n",
        "tokenizer.save_pretrained(\"gurukul_adapter\")\n",
        "\n",
        "# Create ZIP file\n",
        "import shutil\n",
        "shutil.make_archive(\"gurukul_adapter\", 'zip', \"gurukul_adapter\")\n",
        "\n",
        "print(\"‚úÖ Adapter saved!\\n\")\n",
        "print(\"üì¶ Download 'gurukul_adapter.zip' and extract to adapters/gurukul_lite/\\n\")\n",
        "\n",
        "# Provide download link\n",
        "from google.colab import files\n",
        "print(\"‚¨áÔ∏è Click below to download:\")\n",
        "files.download('gurukul_adapter.zip')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ ALL DONE!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Download gurukul_adapter.zip\")\n",
        "print(\"2. Extract to adapters/gurukul_lite/ on your PC\")\n",
        "print(\"3. Test with: python test_colab_adapter.py\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
