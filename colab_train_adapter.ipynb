{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ BLOOMZ-560M LoRA Adapter Training\n",
    "\n",
    "**Train lightweight LoRA adapters on Google Colab (Free GPU!)**\n",
    "\n",
    "## Instructions:\n",
    "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí T4 GPU ‚Üí Save\n",
    "2. **Run Cell 1**: Install dependencies\n",
    "3. **Run Cell 2**: Load model and prepare training\n",
    "4. **Run Cell 3**: Train the adapter!\n",
    "5. **Run Cell 4**: Download trained adapter\n",
    "\n",
    "Then unzip on your computer and place in `adapters/gurukul_lite/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Install Dependencies & Check GPU\n",
    "\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes scipy\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed!\")\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(\"\\n‚úÖ GPU is ready!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: No GPU detected!\")\n",
    "    print(\"   Go to: Runtime ‚Üí Change runtime type ‚Üí T4 GPU ‚Üí Save\")\n",
    "    print(\"   Then re-run this cell.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: Load Model & Prepare Training Data\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION (You can modify these!)\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    'model_name': 'bigscience/bloomz-560m',\n",
    "    'output_dir': 'trained_adapter',\n",
    "    'max_samples': 500,      # Number of training samples\n",
    "    'num_epochs': 3,         # Training epochs\n",
    "    'batch_size': 4,         # Batch size\n",
    "    'learning_rate': 2e-4,   # Learning rate\n",
    "    'max_length': 512,       # Max sequence length\n",
    "    'lora_r': 8,            # LoRA rank (PROVEN)\n",
    "    'lora_alpha': 16,       # LoRA alpha (PROVEN)\n",
    "    'lora_dropout': 0.05,   # LoRA dropout (PROVEN)\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Training Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"   {k}: {v}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAMPLE TRAINING DATA (Replace with your own!)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìä Creating sample multilingual training data...\")\n",
    "\n",
    "# Sample texts (you can replace this with your own data!)\n",
    "sample_texts = [\n",
    "    \"Hello, how are you today?\",\n",
    "    \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç?\",  # Hindi\n",
    "    \"‰Ω†Â•ΩÔºå‰Ω†‰ªäÂ§©ÊÄé‰πàÊ†∑Ôºü\",  # Chinese\n",
    "    \"Bonjour, comment allez-vous?\",  # French\n",
    "    \"Hola, ¬øc√≥mo est√°s?\",  # Spanish\n",
    "    \"„Åì„Çì„Å´„Å°„ÅØ„ÄÅ„ÅäÂÖÉÊ∞ó„Åß„Åô„ÅãÔºü\",  # Japanese\n",
    "    \"–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?\",  # Russian\n",
    "    \"Ol√°, como voc√™ est√°?\",  # Portuguese\n",
    "    \"ŸÖÿ±ÿ≠ÿ®ÿßÿå ŸÉŸäŸÅ ÿ≠ÿßŸÑŸÉÿü\",  # Arabic\n",
    "    \"ÏïàÎÖïÌïòÏÑ∏Ïöî, Ïñ¥ÎñªÍ≤å ÏßÄÎÇ¥ÏÑ∏Ïöî?\",  # Korean\n",
    "]\n",
    "\n",
    "# Repeat to get desired number of samples\n",
    "train_texts = (sample_texts * (CONFIG['max_samples'] // len(sample_texts) + 1))[:CONFIG['max_samples']]\n",
    "\n",
    "# Split into train/validation\n",
    "split_idx = int(len(train_texts) * 0.9)\n",
    "train_data = train_texts[:split_idx]\n",
    "val_data = train_texts[split_idx:]\n",
    "\n",
    "print(f\"‚úÖ Training samples: {len(train_data)}\")\n",
    "print(f\"   Validation samples: {len(val_data)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nü§ñ Loading BLOOMZ-560M model...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 8-bit quantization for memory efficiency\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG['model_name'],\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# APPLY LORA ADAPTERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüîß Applying LoRA adapters with PROVEN configuration...\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=CONFIG['lora_r'],\n",
    "    lora_alpha=CONFIG['lora_alpha'],\n",
    "    lora_dropout=CONFIG['lora_dropout'],\n",
    "    # BLOOM-specific target modules (PROVEN to work!)\n",
    "    target_modules=['query_key_value', 'dense', 'dense_h_to_4h', 'dense_4h_to_h'],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"\\nüìä Trainable Parameters:\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ============================================================================\n",
    "# TOKENIZE DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìù Tokenizing training data...\")\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    tokenized = []\n",
    "    for text in texts:\n",
    "        tokens = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=CONFIG['max_length'],\n",
    "            padding=False,\n",
    "        )\n",
    "        tokenized.append(tokens)\n",
    "    \n",
    "    return Dataset.from_dict({\n",
    "        'input_ids': [t['input_ids'] for t in tokenized],\n",
    "        'attention_mask': [t['attention_mask'] for t in tokenized]\n",
    "    })\n",
    "\n",
    "train_dataset = tokenize_texts(train_data)\n",
    "val_dataset = tokenize_texts(val_data)\n",
    "\n",
    "print(f\"‚úÖ Tokenization complete!\")\n",
    "print(f\"   Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"   Val dataset: {len(val_dataset)} samples\")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked LM\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete! Ready to train.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Train the Adapter!\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéØ STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Training arguments (PROVEN settings)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG['output_dir'],\n",
    "    num_train_epochs=CONFIG['num_epochs'],\n",
    "    per_device_train_batch_size=CONFIG['batch_size'],\n",
    "    per_device_eval_batch_size=CONFIG['batch_size'],\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    warmup_steps=50,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    fp16=False,  # Disable FP16 (PROVEN to avoid issues)\n",
    "    save_total_limit=2,\n",
    "    eval_strategy='steps',\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='none',\n",
    "    dataloader_num_workers=0,  # Important for stability\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train!\n",
    "print(\"\\nüöÄ Training started...\")\n",
    "print(\"   This will take about 5-10 minutes.\\n\")\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéâ TRAINING COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Save the adapter\n",
    "    print(f\"\\nüíæ Saving adapter to {CONFIG['output_dir']}...\")\n",
    "    model.save_pretrained(CONFIG['output_dir'])\n",
    "    tokenizer.save_pretrained(CONFIG['output_dir'])\n",
    "    \n",
    "    print(\"\\n‚úÖ SUCCESS! Adapter saved.\")\n",
    "    print(\"   Run Cell 4 to download it.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed: {e}\")\n",
    "    print(\"\\nTry reducing batch_size or max_samples in Cell 2 and re-run.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: Download Trained Adapter\n",
    "\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì¶ Packaging trained adapter...\")\n",
    "\n",
    "# Zip the adapter directory\n",
    "shutil.make_archive('trained_adapter', 'zip', CONFIG['output_dir'])\n",
    "\n",
    "print(\"‚úÖ Packaged successfully!\")\n",
    "print(\"\\n‚¨áÔ∏è Downloading trained_adapter.zip...\")\n",
    "\n",
    "# Download\n",
    "files.download('trained_adapter.zip')\n",
    "\n",
    "print(\"\\nüéâ DOWNLOAD COMPLETE!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n1. Unzip 'trained_adapter.zip' on your computer\")\n",
    "print(\"2. Place contents in: C:\\\\pc\\\\Project\\\\adapters\\\\gurukul_lite\\\\\")\n",
    "print(\"3. Start your API:\")\n",
    "print(\"   python -m uvicorn adapter_service.standalone_api:app --port 8110\")\n",
    "print(\"\\n4. Test with adapter:\")\n",
    "print('   curl -X POST http://localhost:8110/generate \\\\')\n",
    "print('     -d \\'{\"prompt\":\"Translate to Hindi: Hello\", \"adapter_path\":\"adapters/gurukul_lite\"}\\'')\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Notes\n",
    "\n",
    "### To Use Your Own Data:\n",
    "1. Click the üìÅ folder icon on the left\n",
    "2. Upload your `.txt` files (one text per line)\n",
    "3. Modify Cell 2 to load your files:\n",
    "\n",
    "```python\n",
    "# Replace the sample_texts section with:\n",
    "train_texts = []\n",
    "import os\n",
    "for filename in os.listdir('/content/'):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            lines = [line.strip() for line in f if len(line.strip()) > 10]\n",
    "            train_texts.extend(lines[:CONFIG['max_samples']//10])\n",
    "```\n",
    "\n",
    "### Troubleshooting:\n",
    "- **Out of memory**: Reduce `batch_size` or `max_samples` in Cell 2\n",
    "- **Slow training**: Normal! 500 samples takes ~5-10 minutes\n",
    "- **No GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí Save\n",
    "\n",
    "### What You Get:\n",
    "- **Trained LoRA adapter** (only ~12MB!)\n",
    "- **Ready to use** with your BLOOMZ-560M model\n",
    "- **Multilingual** capabilities enhanced\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
