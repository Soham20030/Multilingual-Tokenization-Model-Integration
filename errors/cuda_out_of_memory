C:\pc\Project>C:/pc/Project/venv/Scripts/activate.bat

(venv) C:\pc\Project>python train.py
üîß Development environment detected

================================================================================
üöÄ Multilingual Tokenization & Inference API v1.0.1
================================================================================
üìç Host: 127.0.0.1:8000
üîß Debug Mode: True
üìä Log Level: INFO
üìù Log File: logs/api.log
üåê Languages: hindi, sanskrit, marathi, english
ü§ñ Model name (fallback): bigscience/bloom-560m
üìö Tokenizer (SentencePiece): model/multi_tokenizer.model
================================================================================
üîß Development environment detected
================================================================================
üöÄ Multilingual Fine-tuning Training v1.0.1
================================================================================
ü§ñ Model: bigscience/bloom-560m
üìÅ Output Directory: mbart_finetuned
üåê Languages: hindi, sanskrit, marathi, english
üìä Training Epochs: 3
üì¶ Batch Size: 4
üéØ Learning Rate: 5e-05
================================================================================
INFO:__main__:Loading tokenizer and model from bigscience/bloom-560m
INFO:__main__:Using max_length: 1024
INFO:__main__:Loading local training data...
INFO:__main__:Loading hindi training data from data/training\hi_train.txt
INFO:__main__:Loaded 1032566 hindi training samples
INFO:__main__:Loading sanskrit training data from data/training\sa_train.txt
INFO:__main__:Loaded 682922 sanskrit training samples
INFO:__main__:Loading marathi training data from data/training\mr_train.txt
INFO:__main__:Loaded 500500 marathi training samples
INFO:__main__:Loading english training data from data/training\en_train.txt
INFO:__main__:Loaded 17013 english training samples
INFO:__main__:Loading hindi validation data from data/validation\hi_val.txt
INFO:__main__:Loaded 258118 hindi validation samples
INFO:__main__:Loading sanskrit validation data from data/validation\sa_val.txt
INFO:__main__:Loaded 170728 sanskrit validation samples
INFO:__main__:Loading marathi validation data from data/validation\mr_val.txt
INFO:__main__:Loaded 125110 marathi validation samples
INFO:__main__:Loading english validation data from data/validation\en_val.txt
INFO:__main__:Loaded 4260 english validation samples
INFO:__main__:Total training samples: 2233001
INFO:__main__:Total validation samples: 558216
INFO:__main__:Tokenizing training dataset...
Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2233001/2233001 [08:21<00:00, 4453.11 examples/s]
INFO:__main__:Tokenizing validation dataset...
Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558216/558216 [02:05<00:00, 4439.48 examples/s]
INFO:__main__:Starting training...
  0%|                                                                                                             | 0/1674750 [00:00<?, ?it/s]Traceback (most recent call last):
  File "C:\pc\Project\train.py", line 272, in <module>
    main()
  File "C:\pc\Project\train.py", line 228, in main
    trainer.train()
  File "C:\pc\Project\venv\Lib\site-packages\transformers\trainer.py", line 2328, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\transformers\trainer.py", line 2672, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\transformers\trainer.py", line 4009, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\transformers\trainer.py", line 4099, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\accelerate\utils\operations.py", line 818, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\accelerate\utils\operations.py", line 806, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\torch\amp\autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\transformers\models\bloom\modeling_bloom.py", line 874, in forward
    transformer_outputs = self.transformer(
                          ^^^^^^^^^^^^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\transformers\models\bloom\modeling_bloom.py", line 586, in forward
    outputs = block(
              ^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\transformers\modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\transformers\models\bloom\modeling_bloom.py", line 403, in forward
    attention_output, attn_weights = self.self_attention(
                                     ^^^^^^^^^^^^^^^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\transformers\models\bloom\modeling_bloom.py", line 301, in forward
    attention_probs = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_layer.dtype)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\pc\Project\venv\Lib\site-packages\torch\nn\functional.py", line 2139, in softmax
    ret = input.softmax(dim, dtype=dtype)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.29 GiB is allocated by PyTorch, and 154.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)