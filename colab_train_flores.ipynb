{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🚀 Train BLOOMZ-560M with FLORES Parallel Translation Data\n",
        "\n",
        "## 📝 What This Does:\n",
        "This notebook trains a LoRA adapter on **high-quality parallel translation data** (FLORES-101).\n",
        "\n",
        "Unlike the previous attempt with monolingual data, this uses **26,117 English↔Indian language translation pairs** so the model learns to actually translate!\n",
        "\n",
        "## 🎯 Expected Results:\n",
        "- ✅ Proper translations (not gibberish)\n",
        "- ✅ Follows \"Translate to [Language]:\" instructions\n",
        "- ✅ Much better quality than base model alone\n",
        "\n",
        "---\n",
        "\n",
        "## 📋 Quick Start:\n",
        "\n",
        "### Step 1: Upload Data to Google Drive\n",
        "1. Upload `flores_training_data.txt` to your Google Drive\n",
        "2. Note the exact file path\n",
        "\n",
        "### Step 2: Enable GPU\n",
        "1. Runtime → Change runtime type → GPU (T4)\n",
        "2. Click Save\n",
        "\n",
        "### Step 3: Run All Cells\n",
        "1. Click Runtime → Run all\n",
        "2. Grant Drive access when prompted\n",
        "3. Wait ~40-60 minutes\n",
        "\n",
        "### Step 4: Download & Test\n",
        "1. Download `gurukul_adapter.zip`\n",
        "2. Extract to `adapters/gurukul_lite/`\n",
        "3. Test with `test_colab_adapter.py`\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Install Dependencies & Check GPU\n",
        "print(\"Installing dependencies...\\n\")\n",
        "!pip install -q transformers datasets peft accelerate bitsandbytes scipy\n",
        "\n",
        "import torch\n",
        "print(f\"\\nGPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"\\nWARNING: No GPU detected!\")\n",
        "    print(\"Go to Runtime -> Change runtime type -> Select GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Mount Google Drive & Load Data\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "print(\"Mounting Google Drive...\\n\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# UPDATE THIS PATH to where you uploaded flores_training_data.txt\n",
        "data_file = \"/content/drive/MyDrive/flores_training_data.txt\"\n",
        "\n",
        "# Check if file exists\n",
        "if not os.path.exists(data_file):\n",
        "    print(f\"\\nERROR: File not found: {data_file}\")\n",
        "    print(\"\\nPlease:\")\n",
        "    print(\"1. Upload flores_training_data.txt to your Google Drive\")\n",
        "    print(\"2. Update the 'data_file' path above to match your file location\")\n",
        "    print(\"3. Re-run this cell\")\n",
        "    raise FileNotFoundError(f\"Missing {data_file}\")\n",
        "\n",
        "print(f\"\\nFound data file!\")\n",
        "print(f\"Size: {os.path.getsize(data_file) / 1e6:.2f} MB\")\n",
        "\n",
        "# Read the file\n",
        "print(\"\\nLoading translation pairs...\")\n",
        "with open(data_file, 'r', encoding='utf-8') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Split into pairs (each pair separated by double newline)\n",
        "pairs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n",
        "\n",
        "print(f\"Loaded {len(pairs):,} translation pairs\")\n",
        "print(f\"\\nSample pairs:\")\n",
        "for i, pair in enumerate(pairs[:3], 1):\n",
        "    lines = pair.split('\\n')\n",
        "    if len(lines) >= 2:\n",
        "        print(f\"\\n{i}. {lines[0][:60]}...\")\n",
        "        print(f\"   {lines[1][:60]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Load Model & Tokenizer\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "\n",
        "print(\"Loading BLOOMZ-560M...\\n\")\n",
        "\n",
        "# Quantization for memory efficiency\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model_name = \"bigscience/bloomz-560m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Set padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Model loaded!\")\n",
        "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Prepare Training Data\n",
        "from datasets import Dataset\n",
        "\n",
        "print(\"Preparing training data...\\n\")\n",
        "\n",
        "# Convert pairs to dataset format\n",
        "texts = []\n",
        "for pair in pairs:\n",
        "    # Each pair is \"Translate to [Lang]: [English]\\n[Translation]\"\n",
        "    # We combine them into a single training text\n",
        "    texts.append(pair)\n",
        "\n",
        "# Create dataset\n",
        "dataset = Dataset.from_dict({'text': texts})\n",
        "\n",
        "print(f\"Dataset created: {len(dataset):,} examples\")\n",
        "print(f\"\\nFirst example:\")\n",
        "print(dataset[0]['text'][:200] + \"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Tokenize Data\n",
        "print(\"Tokenizing data...\\n\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        max_length=256  # Keep it short for translation pairs\n",
        "    )\n",
        "\n",
        "# Tokenize\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['text'],\n",
        "    desc=\"Tokenizing\"\n",
        ")\n",
        "\n",
        "print(f\"Tokenization complete!\")\n",
        "print(f\"Dataset size: {len(tokenized_dataset):,} examples\")\n",
        "print(f\"Sample token length: {len(tokenized_dataset[0]['input_ids'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Apply LoRA Adapter\n",
        "print(\"Applying LoRA adapter...\\n\")\n",
        "\n",
        "# Prepare model\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA configuration (proven settings for BLOOM)\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,  # Rank\n",
        "    lora_alpha=16,  # Alpha\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=['query_key_value', 'dense', 'dense_h_to_4h', 'dense_4h_to_h'],\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(\"LoRA applied!\")\n",
        "print(f\"Trainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "print(f\"Total params: {total_params:,}\")\n",
        "print(f\"\\nAdapter size: ~{trainable_params * 2 / 1e6:.1f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Train the Adapter!\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "print(\"Starting training...\\n\")\n",
        "print(\"OPTIMIZED for speed - should take 45-60 minutes on T4 GPU\\n\")\n",
        "\n",
        "# SPEED OPTIMIZATIONS:\n",
        "# 1. Larger batch size (8 instead of 4) - 2x faster\n",
        "# 2. Fewer epochs (3 instead of 5) - still good quality\n",
        "# 3. More aggressive gradient accumulation\n",
        "# 4. Disabled gradient checkpointing - faster but uses more VRAM\n",
        "# 5. Frequent checkpoints every 500 steps\n",
        "\n",
        "# Training configuration\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./adapter_training\",\n",
        "    num_train_epochs=3,  # Reduced from 5 - still effective\n",
        "    per_device_train_batch_size=8,  # Increased from 4 - 2x faster!\n",
        "    gradient_accumulation_steps=2,  # Effective batch size = 16 (same)\n",
        "    learning_rate=3e-4,  # Slightly higher since fewer epochs\n",
        "    fp16=True,  # Mixed precision for speed\n",
        "    logging_steps=50,  # More frequent logging\n",
        "    save_steps=500,  # CHECKPOINT every 500 steps (~every 6-8 minutes)\n",
        "    save_total_limit=3,  # Keep last 3 checkpoints\n",
        "    warmup_steps=100,  # Reduced warmup\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",\n",
        "    optim=\"adamw_torch\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    gradient_checkpointing=False,  # DISABLED for speed (uses more VRAM but faster)\n",
        "    dataloader_num_workers=2,  # Use 2 workers for faster data loading\n",
        "    dataloader_pin_memory=True,  # Pin memory for faster GPU transfer\n",
        "    max_grad_norm=1.0,  # Gradient clipping for stability\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Train!\n",
        "print(\"=\"*80)\n",
        "print(\"TRAINING INFO:\")\n",
        "print(f\"  Total steps: {len(tokenized_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n",
        "print(f\"  Checkpoint every: {training_args.save_steps} steps (~6-8 minutes)\")\n",
        "print(f\"  Checkpoints saved to: {training_args.output_dir}/\")\n",
        "print(f\"  If training crashes, you can resume from last checkpoint!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nStarting training...\\n\")\n",
        "\n",
        "# Check if we have a checkpoint to resume from\n",
        "import os\n",
        "checkpoints = [d for d in os.listdir(training_args.output_dir) if d.startswith(\"checkpoint-\")] if os.path.exists(training_args.output_dir) else []\n",
        "resume_from_checkpoint = None\n",
        "\n",
        "if checkpoints:\n",
        "    # Sort by step number and get the latest\n",
        "    latest_checkpoint = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))[-1]\n",
        "    resume_from_checkpoint = os.path.join(training_args.output_dir, latest_checkpoint)\n",
        "    print(f\"Found checkpoint: {latest_checkpoint}\")\n",
        "    print(f\"Resuming training from step {latest_checkpoint.split('-')[1]}...\\n\")\n",
        "\n",
        "# Train (will resume if checkpoint found)\n",
        "trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nCheckpoints saved in: {training_args.output_dir}/\")\n",
        "print(\"You can find:\")\n",
        "for checkpoint in sorted(os.listdir(training_args.output_dir)):\n",
        "    if checkpoint.startswith(\"checkpoint-\"):\n",
        "        print(f\"  - {checkpoint}\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 💾 CHECKPOINT MANAGEMENT\n",
        "\n",
        "**Checkpoints are saved every 500 steps (~6-8 minutes).**\n",
        "\n",
        "If training crashes or Colab disconnects:\n",
        "1. Just re-run Cell 7 - it will automatically resume from the last checkpoint!\n",
        "2. No need to start from scratch\n",
        "\n",
        "**To manually download a checkpoint during training:**\n",
        "- Navigate to the Files tab (📁) on the left\n",
        "- Go to `adapter_training/checkpoint-XXXX/`\n",
        "- Right-click → Download (or compress and download)\n",
        "\n",
        "**Each checkpoint contains:**\n",
        "- `adapter_model.safetensors` - The trained adapter weights\n",
        "- `adapter_config.json` - Configuration\n",
        "- `optimizer.pt` - Optimizer state (for resuming)\n",
        "- `trainer_state.json` - Training progress\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Test the Trained Adapter\n",
        "print(\"Testing the trained adapter...\\n\")\n",
        "\n",
        "test_prompts = [\n",
        "    \"Translate to Hindi: Hello friend, how are you?\",\n",
        "    \"Translate to Bengali: Good morning, have a nice day.\",\n",
        "    \"Translate to Tamil: Thank you very much.\",\n",
        "    \"Translate to Telugu: Welcome to our school.\",\n",
        "    \"Translate to Gujarati: How can I help you?\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TEST RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            min_new_tokens=10,\n",
        "            temperature=0.3,  # Lower for more focused output\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            no_repeat_ngram_size=3,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    print(f\"\\n{i}. Prompt: {prompt}\")\n",
        "    print(f\"   Output: {generated}\")\n",
        "    \n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Save & Download Adapter\n",
        "import shutil\n",
        "\n",
        "print(\"Saving adapter...\\n\")\n",
        "\n",
        "# Save adapter\n",
        "output_dir = \"gurukul_flores_adapter\"\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"Adapter saved to {output_dir}/\")\n",
        "\n",
        "# Create ZIP file\n",
        "zip_file = \"gurukul_flores_adapter\"\n",
        "shutil.make_archive(zip_file, 'zip', output_dir)\n",
        "\n",
        "print(f\"\\nCreated {zip_file}.zip\")\n",
        "print(f\"Size: {os.path.getsize(zip_file + '.zip') / 1e6:.2f} MB\")\n",
        "\n",
        "# Download\n",
        "from google.colab import files\n",
        "print(\"\\nDownloading...\")\n",
        "files.download(zip_file + '.zip')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ALL DONE!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Extract gurukul_flores_adapter.zip to adapters/gurukul_lite/ on your PC\")\n",
        "print(\"2. Run: python test_colab_adapter.py\")\n",
        "print(\"3. Compare with base model results!\")\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
