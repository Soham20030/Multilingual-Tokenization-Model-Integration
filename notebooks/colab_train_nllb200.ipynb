{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLLB-200 Adapter Training with FLORES-200\n\n**Train a LoRA adapter for NLLB-200 on 18 Indian languages**\n\n**Languages:** Assamese, Bengali, English, Gujarati, Hindi, Kannada, Kashmiri, Malayalam, Meitei, Marathi, Nepali, Odia, Punjabi, Sanskrit, Sindhi, Tamil, Telugu, Urdu\n\n**Expected Training Time:** 2-3 hours on T4 GPU (20 epochs)\n\n---\n\n## Setup Instructions:\n1. Upload `flores200_dataset.tar.gz` to Colab\n2. Run all cells in order\n3. Download the trained adapter at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Check GPU and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n\nimport torch\nprint(f\"\\nPyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.35.0 datasets==2.14.0 peft==0.6.0 accelerate==0.24.0 bitsandbytes==0.41.1 sentencepiece==0.1.99\n\nprint(\"✅ All packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Upload FLORES-200 Data\n\n**Upload your flores200_dataset.tar.gz file (25MB)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\nimport os\n\nif not os.path.exists('flores200_dataset'):\n    print(\"Please upload flores200_dataset.tar.gz\")\n    uploaded = files.upload()\n    \n    # Extract\n    !tar -xzf flores200_dataset.tar.gz\n    print(\"✅ FLORES-200 extracted!\")\nelse:\n    print(\"✅ FLORES-200 already available!\")\n\n# Verify\n!ls flores200_dataset/dev | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Create Training Dataset from FLORES-200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nfrom pathlib import Path\n\n# Your 17 Indian languages + English\nlanguages = {\n    \"Assamese\": \"asm_Beng\",\n    \"Bengali\": \"ben_Beng\",\n    \"Gujarati\": \"guj_Gujr\",\n    \"Hindi\": \"hin_Deva\",\n    \"Kannada\": \"kan_Knda\",\n    \"Kashmiri\": \"kas_Arab\",\n    \"Malayalam\": \"mal_Mlym\",\n    \"Meitei\": \"mni_Beng\",\n    \"Marathi\": \"mar_Deva\",\n    \"Nepali\": \"npi_Deva\",\n    \"Odia\": \"ory_Orya\",\n    \"Punjabi\": \"pan_Guru\",\n    \"Sanskrit\": \"san_Deva\",\n    \"Sindhi\": \"snd_Arab\",\n    \"Tamil\": \"tam_Taml\",\n    \"Telugu\": \"tel_Telu\",\n    \"Urdu\": \"urd_Arab\",\n}\n\n# Create parallel translation pairs\noutput_file = \"flores200_training.txt\"\n\nprint(\"Creating training data from FLORES-200...\")\n\nwith open(output_file, 'w', encoding='utf-8') as out:\n    total_pairs = 0\n    \n    # Process both dev and devtest splits\n    for split in ['dev', 'devtest']:\n        # Read English sentences\n        eng_file = f\"flores200_dataset/{split}/eng_Latn.{split}\"\n        with open(eng_file, 'r', encoding='utf-8') as f:\n            english_sentences = [line.strip() for line in f.readlines()]\n        \n        print(f\"  {split}: {len(english_sentences)} English sentences\")\n        \n        # For each target language\n        for lang_name, lang_code in languages.items():\n            target_file = f\"flores200_dataset/{split}/{lang_code}.{split}\"\n            \n            with open(target_file, 'r', encoding='utf-8') as f:\n                target_sentences = [line.strip() for line in f.readlines()]\n            \n            # Create parallel pairs (English -> Target)\n            for eng, target in zip(english_sentences, target_sentences):\n                if eng.strip() and target.strip():\n                    out.write(f\"{eng}\\n\")\n                    out.write(f\"{target}\\n\")\n                    out.write(\"\\n\")\n                    total_pairs += 1\n\nprint(f\"\\n✅ Created {total_pairs:,} parallel translation pairs\")\nprint(f\"✅ File: {output_file}\")\nprint(f\"✅ Size: {Path(output_file).stat().st_size / 1024 / 1024:.1f} MB\")\nprint(f\"✅ Languages: 17 Indian + English\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Load NLLB-200 Model (8-bit quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    BitsAndBytesConfig,\n)\nfrom peft import prepare_model_for_kbit_training\nimport torch\n\nprint(\"Loading NLLB-200-distilled-600M...\\n\")\n\nmodel_name = \"facebook/nllb-200-distilled-600M\"\n\n# 8-bit quantization config for memory efficiency\nbnb_config = BitsAndBytesConfig(\n    load_in_8bit=True,\n    bnb_8bit_compute_dtype=torch.float16\n)\n\n# Load tokenizer\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Load model in 8-bit\nprint(\"Loading model (this may take 2-3 minutes)...\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\n# Prepare for LoRA training\nprint(\"Preparing model for training...\")\nmodel = prepare_model_for_kbit_training(model)\n\nprint(f\"\\n✅ Model loaded: {model_name}\")\nprint(f\"✅ Model size: ~600M parameters\")\nprint(f\"✅ Quantization: 8-bit (saves memory)\")\nprint(f\"✅ Ready for LoRA adapter training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Configure LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n\n# LoRA configuration for NLLB (Seq2Seq translation model)\nlora_config = LoraConfig(\n    r=16,                              # Rank (adapter capacity)\n    lora_alpha=32,                     # Scaling factor\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],  # NLLB attention layers\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM    # Translation task\n)\n\n# Apply LoRA to model\nprint(\"Applying LoRA adapter...\")\nmodel = get_peft_model(model, lora_config)\n\n# Show trainable parameters\nprint(\"\\n\" + \"=\"*60)\nmodel.print_trainable_parameters()\nprint(\"=\"*60)\n\nprint(\"\\n✅ LoRA adapter configured!\")\nprint(\"✅ Only ~1-2% of parameters will be trained (very efficient!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\nfrom transformers import DataCollatorForSeq2Seq\n\n# Read training file\nprint(\"Loading training data...\")\nwith open(\"flores200_training.txt\", 'r', encoding='utf-8') as f:\n    lines = f.readlines()\n\n# Parse parallel pairs\nsources = []\ntargets = []\n\ni = 0\nwhile i < len(lines):\n    source = lines[i].strip()\n    if i + 1 < len(lines):\n        target = lines[i + 1].strip()\n        if source and target:\n            sources.append(source)\n            targets.append(target)\n    i += 3  # Skip empty line\n\nprint(f\"✅ Loaded {len(sources):,} parallel pairs\\n\")\n\n# Create dataset\ndataset_dict = {\"source\": sources, \"target\": targets}\ndataset = Dataset.from_dict(dataset_dict)\n\n# Tokenize function\ndef tokenize_function(examples):\n    # Tokenize source\n    model_inputs = tokenizer(\n        examples[\"source\"],\n        max_length=128,\n        truncation=True,\n        padding=False  # Dynamic padding during training\n    )\n    \n    # Tokenize target\n    labels = tokenizer(\n        examples[\"target\"],\n        max_length=128,\n        truncation=True,\n        padding=False\n    )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Tokenize dataset\nprint(\"Tokenizing dataset (this may take 1-2 minutes)...\")\ntokenized_dataset = dataset.map(\n    tokenize_function,\n    batched=True,\n    batch_size=1000,\n    remove_columns=[\"source\", \"target\"]\n)\n\nprint(f\"✅ Tokenized {len(tokenized_dataset):,} examples\")\n\n# Data collator for dynamic padding\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=True\n)\n\nprint(\"✅ Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./nllb_adapter\",\n    \n    # Training duration\n    num_train_epochs=20,               # 20 epochs for good quality\n    \n    # Batch size (optimized for T4 GPU)\n    per_device_train_batch_size=4,     # 4 works well on T4\n    gradient_accumulation_steps=4,     # Effective batch = 16\n    \n    # Optimizer\n    learning_rate=2e-4,\n    warmup_steps=500,\n    \n    # Saving checkpoints\n    save_strategy=\"epoch\",\n    save_total_limit=3,                # Keep last 3 checkpoints\n    \n    # Logging\n    logging_steps=100,\n    logging_dir=\"./logs\",\n    \n    # Performance\n    fp16=False,                        # Use bf16 on T4\n    bf16=True,\n    dataloader_num_workers=2,          # Parallel data loading\n    \n    # Seq2Seq specific\n    predict_with_generate=False,       # Faster training\n    \n    # Memory optimization\n    gradient_checkpointing=False,      # Faster but uses more memory\n)\n\nprint(\"✅ Training configuration ready\")\nprint(f\"\\n📊 Training Details:\")\nprint(f\"   • Epochs: {training_args.num_train_epochs}\")\nprint(f\"   • Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"   • Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"   • Effective batch: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"   • Learning rate: {training_args.learning_rate}\")\nprint(f\"   • Estimated time: 2-3 hours on T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Train the Adapter\n\n**⏱️ This will take 2-3 hours on T4 GPU. You can close the browser but keep the tab open!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\nimport time\n\n# Create trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator,\n)\n\nprint(\"=\"*80)\nprint(\"🚀 STARTING TRAINING\")\nprint(\"=\"*80)\nprint(f\"📊 Total samples: {len(tokenized_dataset):,}\")\nprint(f\"📊 Epochs: {training_args.num_train_epochs}\")\nprint(f\"📊 Languages: 17 Indian languages + English\")\nprint(f\"⏱️  Estimated time: 2-3 hours\")\nprint()\nprint(\"💡 Tip: You can minimize the browser but keep the tab open!\")\nprint(\"=\"*80)\nprint()\n\nstart_time = time.time()\n\n# Train!\ntrainer.train()\n\nend_time = time.time()\nduration_minutes = (end_time - start_time) / 60\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"✅ TRAINING COMPLETE!\")\nprint(\"=\"*80)\nprint(f\"⏱️  Total time: {duration_minutes:.1f} minutes ({duration_minutes/60:.1f} hours)\")\nprint(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Save the Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final adapter\noutput_adapter_dir = \"nllb_18languages_adapter\"\n\nprint(\"Saving adapter...\")\nmodel.save_pretrained(output_adapter_dir)\ntokenizer.save_pretrained(output_adapter_dir)\n\nprint(f\"\\n✅ Adapter saved to: {output_adapter_dir}\")\nprint(\"\\nFiles saved:\")\n!ls -lh {output_adapter_dir}\n\nimport os\nsize_mb = sum(os.path.getsize(os.path.join(output_adapter_dir, f)) for f in os.listdir(output_adapter_dir)) / 1024 / 1024\nprint(f\"\\n📦 Total size: {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Test the Adapter\n\n**Let's test translations in a few languages!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing adapter with sample translations...\\n\")\n\ntest_sentences = [\n    \"Hello, how are you today?\",\n    \"This is a beautiful day.\",\n    \"Thank you for your help.\",\n    \"Machine learning is fascinating.\",\n    \"I love reading books.\",\n]\n\nprint(\"=\"*80)\nfor idx, sentence in enumerate(test_sentences, 1):\n    print(f\"\\n🔤 Test {idx}: {sentence}\")\n    \n    # Tokenize\n    inputs = tokenizer(sentence, return_tensors=\"pt\").to(model.device)\n    \n    # Generate translation\n    outputs = model.generate(\n        **inputs,\n        max_length=128,\n        num_beams=4,\n        early_stopping=True\n    )\n    \n    # Decode\n    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"   ➜ Translation: {translation}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"✅ Adapter is working!\")\nprint(\"\\n💡 Note: The model translates to various languages in the training data.\")\nprint(\"    You can control the target language using NLLB language codes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Download the Adapter\n\n**Download and extract on your PC!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the adapter for download\nprint(\"Zipping adapter...\")\n!zip -r nllb_18languages_adapter.zip nllb_18languages_adapter/\n\nprint(\"\\n✅ Adapter zipped!\")\n\n# Show size\nimport os\nsize_mb = os.path.getsize(\"nllb_18languages_adapter.zip\") / 1024 / 1024\nprint(f\"📦 Download size: {size_mb:.1f} MB\")\nprint()\n\n# Download\nfrom google.colab import files\nprint(\"Starting download...\")\nfiles.download(\"nllb_18languages_adapter.zip\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🎉 COMPLETE!\")\nprint(\"=\"*80)\nprint()\nprint(\"📥 Next steps:\")\nprint(\"   1. Extract nllb_18languages_adapter.zip on your PC\")\nprint(\"   2. Place it in: adapters/nllb_18languages_adapter/\")\nprint(\"   3. Update standalone_api.py to use:\")\nprint(\"      • base_model: 'facebook/nllb-200-distilled-600M'\")\nprint(\"      • adapter_path: 'adapters/nllb_18languages_adapter'\")\nprint(\"   4. Test with your 18 languages!\")\nprint()\nprint(\"✨ Expected quality: 85-90% accuracy for all 18 languages!\")\nprint(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}