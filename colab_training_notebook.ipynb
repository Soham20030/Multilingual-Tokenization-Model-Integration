{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Multilingual Fine-tuning Training Script for Google Colab\n",
        "\n",
        "This notebook fine-tunes a language model on multilingual data (Hindi, Sanskrit, Marathi, English) using LoRA/PEFT for efficient training.\n",
        "\n",
        "## Features:\n",
        "- ‚úÖ All dependencies included\n",
        "- ‚úÖ Sample data generation for demo\n",
        "- ‚úÖ Memory-optimized for Colab's GPU constraints\n",
        "- ‚úÖ Automatic GPU detection and configuration\n",
        "- ‚úÖ Progress tracking and logging\n",
        "- ‚úÖ LoRA/PEFT support for efficient training\n",
        "\n",
        "## Usage:\n",
        "1. Enable GPU in Colab (Runtime > Change runtime type > GPU)\n",
        "2. Run each cell in sequence\n",
        "3. Download the fine-tuned model when complete\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "%pip install -q transformers datasets accelerate peft bitsandbytes\n",
        "%pip install -q sentencepiece langdetect\n",
        "\n",
        "# Verify installation\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries and setup\n",
        "import logging\n",
        "import os\n",
        "import gc\n",
        "import hashlib\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = \"AhinsaAI/ahinsa0.5-llama3.2-3B\"  # Change this to your preferred model\n",
        "OUTPUT_DIR = \"fine_tuned_model\"\n",
        "EPOCHS = 2\n",
        "BATCH_SIZE = 1\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "WARMUP_STEPS = 100\n",
        "LEARNING_RATE = 5e-5\n",
        "MAX_LENGTH = 512\n",
        "USE_QUANTIZATION = True\n",
        "USE_PEFT = True\n",
        "\n",
        "print(f\"ü§ñ Model: {MODEL_NAME}\")\n",
        "print(f\"üìä Training Epochs: {EPOCHS}\")\n",
        "print(f\"üîß Quantization: {'Enabled' if USE_QUANTIZATION else 'Disabled'}\")\n",
        "print(f\"üîß PEFT/LoRA: {'Enabled' if USE_PEFT else 'Disabled'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility functions\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory and run garbage collection\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "    gc.collect()\n",
        "\n",
        "def check_gpu_usage():\n",
        "    \"\"\"Check and log GPU usage\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.cuda.current_device()\n",
        "        gpu_name = torch.cuda.get_device_name(device)\n",
        "        memory_allocated = torch.cuda.memory_allocated(device) / 1024**3\n",
        "        memory_reserved = torch.cuda.memory_reserved(device) / 1024**3\n",
        "        memory_total = torch.cuda.get_device_properties(device).total_memory / 1024**3\n",
        "        \n",
        "        logger.info(f\"GPU: {gpu_name}\")\n",
        "        logger.info(f\"GPU Memory - Allocated: {memory_allocated:.2f} GB, Reserved: {memory_reserved:.2f} GB, Total: {memory_total:.2f} GB\")\n",
        "        return True\n",
        "    else:\n",
        "        logger.info(\"CUDA not available - using CPU\")\n",
        "        return False\n",
        "\n",
        "# Check GPU\n",
        "check_gpu_usage()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample multilingual training data\n",
        "def create_sample_data():\n",
        "    \"\"\"Create sample multilingual training data for demonstration\"\"\"\n",
        "    \n",
        "    sample_data = {\n",
        "        \"hindi\": [\n",
        "            \"‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§π‡•Ç‡§Ç ‡§î‡§∞ ‡§Æ‡•Å‡§ù‡•á ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§≠‡§æ‡§∑‡§æ ‡§∏‡•Ä‡§ñ‡§®‡§æ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§≤‡§ó‡§§‡§æ ‡§π‡•à‡•§\",\n",
        "            \"‡§Ø‡§π ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡§¨‡§π‡•Å‡§§ ‡§∏‡•Å‡§Ç‡§¶‡§∞ ‡§π‡•à ‡§î‡§∞ ‡§π‡§Æ‡•á‡§Ç ‡§á‡§∏‡•á ‡§∏‡§Ç‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è‡•§\",\n",
        "            \"‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§∏‡§¨‡§∏‡•á ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§ö‡•Ä‡§ú ‡§π‡•à ‡§ú‡•ã ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø ‡§ï‡•ã ‡§∏‡§´‡§≤ ‡§¨‡§®‡§æ ‡§∏‡§ï‡§§‡•Ä ‡§π‡•à‡•§\",\n",
        "            \"‡§™‡•ç‡§∞‡•á‡§Æ ‡§î‡§∞ ‡§ï‡§∞‡•Å‡§£‡§æ ‡§π‡§Æ‡§æ‡§∞‡•á ‡§ú‡•Ä‡§µ‡§® ‡§ï‡•ã ‡§∏‡•Å‡§Ç‡§¶‡§∞ ‡§¨‡§®‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§\",\n",
        "            \"‡§≠‡§æ‡§∞‡§§ ‡§µ‡§ø‡§µ‡§ø‡§ß‡§§‡§æ ‡§Æ‡•á‡§Ç ‡§è‡§ï‡§§‡§æ ‡§ï‡§æ ‡§¶‡•á‡§∂ ‡§π‡•à‡•§\"\n",
        "        ],\n",
        "        \"sanskrit\": [\n",
        "            \"‡§∏‡§∞‡•ç‡§µ‡•á ‡§≠‡§µ‡§®‡•ç‡§§‡•Å ‡§∏‡•Å‡§ñ‡§ø‡§®‡§É ‡§∏‡§∞‡•ç‡§µ‡•á ‡§∏‡§®‡•ç‡§§‡•Å ‡§®‡§ø‡§∞‡§æ‡§Æ‡§Ø‡§æ‡§É‡•§\",\n",
        "            \"‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ ‡§¶‡§¶‡§æ‡§§‡§ø ‡§µ‡§ø‡§®‡§Ø‡§Ç ‡§µ‡§ø‡§®‡§Ø‡§æ‡§¶‡•ç‡§Ø‡§æ‡§§‡§ø ‡§™‡§æ‡§§‡•ç‡§∞‡§§‡§æ‡§Æ‡•ç‡•§\",\n",
        "            \"‡§∏‡§§‡•ç‡§Ø‡§Æ‡•á‡§µ ‡§ú‡§Ø‡§§‡•á ‡§®‡§æ‡§®‡•É‡§§‡§Ç ‡§∏‡§§‡•ç‡§Ø‡•á‡§® ‡§™‡§®‡•ç‡§•‡§æ ‡§µ‡§ø‡§§‡§§‡•ã ‡§¶‡•á‡§µ‡§Ø‡§æ‡§®‡§É‡•§\",\n",
        "            \"‡§Ö‡§π‡§ø‡§Ç‡§∏‡§æ ‡§™‡§∞‡§Æ‡•ã ‡§ß‡§∞‡•ç‡§Æ‡§É ‡§ß‡§∞‡•ç‡§Æ‡§∏‡•ç‡§Ø ‡§™‡•ç‡§∞‡§§‡§ø‡§∑‡•ç‡§†‡§æ‡•§\",\n",
        "            \"‡§µ‡§∏‡•Å‡§ß‡•à‡§µ ‡§ï‡•Å‡§ü‡•Å‡§Æ‡•ç‡§¨‡§ï‡§Æ‡•ç ‡§á‡§§‡§ø ‡§∏‡•Å‡§≠‡§æ‡§∑‡§ø‡§§‡§Æ‡•ç‡•§\"\n",
        "        ],\n",
        "        \"marathi\": [\n",
        "            \"‡§Æ‡•Ä ‡§è‡§ï ‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Ø ‡§Ü‡§π‡•á ‡§Ü‡§£‡§ø ‡§Æ‡§≤‡§æ ‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§≠‡§æ‡§∑‡§æ ‡§Ü‡§µ‡§°‡§§‡•á‡•§\",\n",
        "            \"‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§π‡•á ‡§Æ‡§æ‡§£‡§∏‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§ú‡•Ä‡§µ‡§®‡§æ‡§§‡•Ä‡§≤ ‡§∏‡§∞‡•ç‡§µ‡§æ‡§§ ‡§Æ‡§π‡§§‡•ç‡§µ‡§æ‡§ö‡•Ä ‡§ó‡•ã‡§∑‡•ç‡§ü ‡§Ü‡§π‡•á‡•§\",\n",
        "            \"‡§™‡•ç‡§∞‡•á‡§Æ ‡§Ü‡§£‡§ø ‡§ï‡§∞‡•Å‡§£‡§æ ‡§Ø‡§æ‡§Ç‡§®‡•Ä ‡§ú‡§ó ‡§∏‡•Å‡§Ç‡§¶‡§∞ ‡§¨‡§®‡§§‡•ã‡•§\",\n",
        "            \"‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§π‡§æ ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡•Ä‡§ö‡§æ ‡§Ü‡§£‡§ø ‡§™‡§∞‡§Ç‡§™‡§∞‡•á‡§ö‡§æ ‡§ó‡•å‡§∞‡§µ‡§∂‡§æ‡§≤‡•Ä ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§Ü‡§π‡•á‡•§\",\n",
        "            \"‡§è‡§ï‡§§‡§æ ‡§π‡•Ä ‡§∂‡§ï‡•ç‡§§‡•Ä‡§ö‡§æ ‡§∏‡•ç‡§∞‡•ã‡§§ ‡§Ü‡§π‡•á‡•§\"\n",
        "        ],\n",
        "        \"english\": [\n",
        "            \"I am learning multiple languages to understand different cultures better.\",\n",
        "            \"Education is the key to success and personal development.\",\n",
        "            \"Love and compassion make the world a better place to live.\",\n",
        "            \"India is a diverse country with unity in diversity.\",\n",
        "            \"Technology has revolutionized the way we communicate and learn.\"\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    # Create directories\n",
        "    os.makedirs(\"data/training\", exist_ok=True)\n",
        "    os.makedirs(\"data/validation\", exist_ok=True)\n",
        "    \n",
        "    # Write sample data to files\n",
        "    for lang, texts in sample_data.items():\n",
        "        # Training data\n",
        "        train_file = f\"data/training/{lang}_train.txt\"\n",
        "        with open(train_file, 'w', encoding='utf-8') as f:\n",
        "            for text in texts:\n",
        "                f.write(text + \"\\n\")\n",
        "        \n",
        "        # Validation data (smaller subset)\n",
        "        val_file = f\"data/validation/{lang}_val.txt\"\n",
        "        with open(val_file, 'w', encoding='utf-8') as f:\n",
        "            for text in texts[:2]:  # Use first 2 samples for validation\n",
        "                f.write(text + \"\\n\")\n",
        "    \n",
        "    print(\"‚úÖ Sample data created successfully!\")\n",
        "    print(\"üìÅ Training data files:\")\n",
        "    for lang in sample_data.keys():\n",
        "        train_file = f\"data/training/{lang}_train.txt\"\n",
        "        val_file = f\"data/validation/{lang}_val.txt\"\n",
        "        print(f\"  - {train_file}\")\n",
        "        print(f\"  - {val_file}\")\n",
        "\n",
        "# Create sample data\n",
        "create_sample_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and tokenizer with optimizations\n",
        "def load_model_and_tokenizer():\n",
        "    \"\"\"Load model and tokenizer with memory optimizations\"\"\"\n",
        "    \n",
        "    clear_gpu_memory()\n",
        "    \n",
        "    # Load tokenizer\n",
        "    logger.info(f\"Loading tokenizer from {MODEL_NAME}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    \n",
        "    # Configure quantization\n",
        "    quantization_config = None\n",
        "    if USE_QUANTIZATION and torch.cuda.is_available():\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_8bit=True,\n",
        "            llm_int8_threshold=6.0,\n",
        "            llm_int8_has_fp16_weight=False,\n",
        "        )\n",
        "        logger.info(\"üîß Using 8-bit quantization\")\n",
        "    \n",
        "    # Load model\n",
        "    logger.info(f\"Loading model from {MODEL_NAME}\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "        low_cpu_mem_usage=True,\n",
        "        torch_dtype=torch.float16 if quantization_config else torch.float32,\n",
        "    )\n",
        "    \n",
        "    # Apply LoRA if enabled\n",
        "    if USE_PEFT:\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            inference_mode=False,\n",
        "            r=16,\n",
        "            lora_alpha=32,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        )\n",
        "        \n",
        "        model = get_peft_model(model, lora_config)\n",
        "        logger.info(\"üîß Applied LoRA adapters\")\n",
        "        model.print_trainable_parameters()\n",
        "    \n",
        "    # Add padding token\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        logger.info(\"Added EOS token as padding token\")\n",
        "    \n",
        "    return model, tokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = load_model_and_tokenizer()\n",
        "check_gpu_usage()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare training data\n",
        "def load_training_data():\n",
        "    \"\"\"Load and tokenize training data\"\"\"\n",
        "    \n",
        "    # Corpus files mapping\n",
        "    corpus_files = {\n",
        "        \"hindi\": \"hindi_train.txt\",\n",
        "        \"sanskrit\": \"sanskrit_train.txt\", \n",
        "        \"marathi\": \"marathi_train.txt\",\n",
        "        \"english\": \"english_train.txt\"\n",
        "    }\n",
        "    \n",
        "    # Load training data\n",
        "    train_texts = []\n",
        "    for lang, filename in corpus_files.items():\n",
        "        filepath = os.path.join(\"data/training\", filename)\n",
        "        if os.path.exists(filepath):\n",
        "            logger.info(f\"Loading {lang} training data from {filepath}\")\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                lines = f.readlines()\n",
        "                filtered_lines = [line.strip() for line in lines if len(line.strip()) > 10]\n",
        "                train_texts.extend(filtered_lines)\n",
        "                logger.info(f\"Loaded {len(filtered_lines)} {lang} training samples\")\n",
        "    \n",
        "    # Load validation data\n",
        "    eval_texts = []\n",
        "    val_files = {\n",
        "        \"hindi\": \"hindi_val.txt\",\n",
        "        \"sanskrit\": \"sanskrit_val.txt\", \n",
        "        \"marathi\": \"marathi_val.txt\",\n",
        "        \"english\": \"english_val.txt\"\n",
        "    }\n",
        "    \n",
        "    for lang, filename in val_files.items():\n",
        "        filepath = os.path.join(\"data/validation\", filename)\n",
        "        if os.path.exists(filepath):\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                lines = f.readlines()\n",
        "                filtered_lines = [line.strip() for line in lines if len(line.strip()) > 10]\n",
        "                eval_texts.extend(filtered_lines)\n",
        "    \n",
        "    logger.info(f\"Total training samples: {len(train_texts)}\")\n",
        "    logger.info(f\"Total validation samples: {len(eval_texts)}\")\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = Dataset.from_dict({\"text\": train_texts})\n",
        "    eval_dataset = Dataset.from_dict({\"text\": eval_texts}) if eval_texts else None\n",
        "    \n",
        "    return train_dataset, eval_dataset\n",
        "\n",
        "# Load and tokenize data\n",
        "train_dataset, eval_dataset = load_training_data()\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"], \n",
        "        truncation=True, \n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=True,\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "# Tokenize datasets\n",
        "logger.info(\"Tokenizing training dataset...\")\n",
        "tokenized_train = train_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "tokenized_eval = None\n",
        "if eval_dataset:\n",
        "    logger.info(\"Tokenizing validation dataset...\")\n",
        "    tokenized_eval = eval_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "logger.info(\"‚úÖ Data tokenization completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup training\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        "    pad_to_multiple_of=8,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    logging_steps=50,\n",
        "    save_steps=200,\n",
        "    eval_steps=200,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=False,\n",
        "    dataloader_drop_last=True,\n",
        "    dataloader_pin_memory=False,\n",
        "    report_to=None,\n",
        "    dataloader_num_workers=0,\n",
        "    save_total_limit=2,\n",
        "    max_grad_norm=1.0,\n",
        "    save_strategy=\"steps\" if USE_PEFT else \"epoch\",\n",
        "    eval_strategy=\"steps\" if USE_PEFT else \"no\",\n",
        "    load_best_model_at_end=True if USE_PEFT else False,\n",
        "    remove_unused_columns=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Add progress callback\n",
        "from transformers import TrainerCallback\n",
        "class ProgressCallback(TrainerCallback):\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if state.global_step % 10 == 0:\n",
        "            logger.info(f\"Training step {state.global_step}/{state.max_steps} - Loss: {state.log_history[-1].get('train_loss', 'N/A') if state.log_history else 'N/A'}\")\n",
        "\n",
        "trainer.add_callback(ProgressCallback())\n",
        "\n",
        "print(\"‚úÖ Training setup completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute training\n",
        "clear_gpu_memory()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    logger.info(f\"GPU memory before training: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "\n",
        "logger.info(\"üöÄ Starting training...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "    logger.info(\"‚úÖ Training completed successfully!\")\n",
        "except torch.cuda.OutOfMemoryError as e:\n",
        "    logger.error(f\"CUDA out of memory error: {e}\")\n",
        "    logger.info(\"Try reducing BATCH_SIZE further or MAX_LENGTH\")\n",
        "    clear_gpu_memory()\n",
        "    raise\n",
        "except Exception as e:\n",
        "    logger.error(f\"Training error: {e}\")\n",
        "    clear_gpu_memory()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model and create download package\n",
        "logger.info(f\"Saving model to {OUTPUT_DIR}\")\n",
        "if USE_PEFT:\n",
        "    model.save_pretrained(OUTPUT_DIR)\n",
        "    logger.info(\"‚úÖ Saved LoRA adapters\")\n",
        "else:\n",
        "    trainer.save_model()\n",
        "\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Create zip file for download\n",
        "import zipfile\n",
        "zip_filename = f\"{OUTPUT_DIR}.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, dirs, files in os.walk(OUTPUT_DIR):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, OUTPUT_DIR)\n",
        "            zipf.write(file_path, arcname)\n",
        "\n",
        "logger.info(f\"‚úÖ Model saved and packaged as {zip_filename}\")\n",
        "\n",
        "# Download the model\n",
        "from google.colab import files\n",
        "print(f\"\\nüì• Download your trained model:\")\n",
        "files.download(zip_filename)\n",
        "\n",
        "print(f\"\\nüéâ Training completed! Model saved to: {OUTPUT_DIR}\")\n",
        "print(f\"üì¶ Download package: {zip_filename}\")\n",
        "print(\"ü§ñ You can now use this model for inference!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the fine-tuned model (Optional)\n",
        "def test_model():\n",
        "    \"\"\"Test the fine-tuned model with sample prompts\"\"\"\n",
        "    \n",
        "    # Test prompts in different languages\n",
        "    test_prompts = [\n",
        "        \"‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§π‡•Ç‡§Ç\",  # Hindi\n",
        "        \"‡§∏‡§∞‡•ç‡§µ‡•á ‡§≠‡§µ‡§®‡•ç‡§§‡•Å ‡§∏‡•Å‡§ñ‡§ø‡§®‡§É\",  # Sanskrit\n",
        "        \"‡§Æ‡•Ä ‡§è‡§ï ‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Ø ‡§Ü‡§π‡•á\",  # Marathi\n",
        "        \"I am learning multiple languages\"  # English\n",
        "    ]\n",
        "    \n",
        "    print(\"üß™ Testing fine-tuned model with sample prompts:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for prompt in test_prompts:\n",
        "        print(f\"\\nüìù Prompt: {prompt}\")\n",
        "        \n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "        \n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=50,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # Decode response\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(f\"ü§ñ Response: {response}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Test the model\n",
        "test_model()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
